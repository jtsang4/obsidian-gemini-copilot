/**
 * Simplified Gemini API implementation using js-genai SDK
 *
 * This replaces the complex API abstraction layer with a single,
 * streamlined implementation powered by @google/genai.
 */

import {
  type Content,
  type FunctionDeclaration,
  type GenerateContentParameters,
  type GenerateContentResponse,
  type GroundingSupport,
  GoogleGenAI,
  type Part,
  type Schema,
  Type,
} from '@google/genai';
import type ObsidianGemini from '../main';
import { GeminiPrompts } from '../prompts';
import type {
  BaseModelRequest,
  ExtendedModelRequest,
  ModelApi,
  ModelResponse,
  StreamCallback,
  StreamingModelResponse,
  ToolCall,
} from './interfaces/model-api';

// Internal conversation entry types
interface ConversationEntryWithText {
  role: 'user' | 'model';
  text: string;
}

interface ConversationEntryWithMessage {
  role: 'user' | 'model';
  message: string;
}

// Gemini API configuration type
interface GeminiGenerationConfig {
  temperature?: number;
  topP?: number;
  maxOutputTokens?: number;
  systemInstruction?: string;
  tools?: Array<{
    functionDeclarations: Array<{
      name: string;
      description: string;
      parameters: {
        type: Type.OBJECT;
        properties: Record<string, Schema>;
        required: string[];
      };
    }>;
  }>;
}

// Extended response type for grounding metadata
interface ExtendedGenerateContentResponse extends Omit<GenerateContentResponse, 'candidates'> {
  candidates?: Array<{
    content?: {
      parts?: Part[];
    };
    groundingMetadata?: {
      groundingChunks?: Array<{
        web?: {
          uri?: string;
          title?: string;
        };
      }>;
      groundingSupports?: GroundingSupport[];
    };
  }>;
}

// Extended Part type for text access
interface ExtendedPart extends Part {
  text?: string;
}

/**
 * Configuration for GeminiClient
 */
export interface GeminiClientConfig {
  apiKey: string;
  temperature?: number;
  topP?: number;
  maxOutputTokens?: number;
  streamingEnabled?: boolean;
}

/**
 * GeminiClient - Simplified API wrapper using js-genai SDK
 *
 * Implements ModelApi interface while leveraging the official Google SDK
 */
export class GeminiClient implements ModelApi {
  private ai: GoogleGenAI;
  private config: GeminiClientConfig;
  private prompts: GeminiPrompts;
  private plugin?: ObsidianGemini;

  constructor(config: GeminiClientConfig, prompts?: GeminiPrompts, plugin?: ObsidianGemini) {
    this.config = {
      temperature: 1.0,
      topP: 0.95,
      streamingEnabled: true,
      ...config,
    };
    this.plugin = plugin;
    this.prompts = prompts || new GeminiPrompts(plugin);
    this.ai = new GoogleGenAI({ apiKey: config.apiKey });
  }

  /**
   * Generate a non-streaming response
   */
  async generateModelResponse(request: BaseModelRequest | ExtendedModelRequest): Promise<ModelResponse> {
    const params = await this.buildGenerateContentParams(request);

    try {
      const response = await this.ai.models.generateContent(params);
      return this.extractModelResponse(response);
    } catch (error) {
      console.error('[GeminiClient] Error generating content:', error);
      throw error;
    }
  }

  /**
   * Generate a streaming response
   */
  generateStreamingResponse(
    request: BaseModelRequest | ExtendedModelRequest,
    onChunk: StreamCallback
  ): StreamingModelResponse {
    let cancelled = false;
    let accumulatedText = '';
    let accumulatedRendered = '';
    let toolCalls: ToolCall[] | undefined;

    const complete = (async (): Promise<ModelResponse> => {
      const params = await this.buildGenerateContentParams(request);

      try {
        const stream = await this.ai.models.generateContentStream(params);

        for await (const chunk of stream) {
          if (cancelled) {
            break;
          }

          // Extract text from chunk
          const chunkText = this.extractTextFromChunk(chunk);
          if (chunkText) {
            accumulatedText += chunkText;
            onChunk(chunkText);
          }

          // Extract tool calls from chunk (usually in last chunk)
          const chunkToolCalls = this.extractToolCallsFromChunk(chunk);
          if (chunkToolCalls?.length) {
            toolCalls = chunkToolCalls;
          }

          // Extract search grounding (rendered HTML)
          const rendered = this.extractRenderedFromChunk(chunk);
          if (rendered) {
            accumulatedRendered += rendered;
          }
        }

        return {
          markdown: accumulatedText,
          rendered: accumulatedRendered,
          ...(toolCalls && { toolCalls }),
        };
      } catch (error) {
        if (cancelled) {
          return {
            markdown: accumulatedText,
            rendered: accumulatedRendered,
            ...(toolCalls && { toolCalls }),
          };
        }
        console.error('[GeminiClient] Streaming error:', error);
        throw error;
      }
    })();

    return {
      complete,
      cancel: () => {
        cancelled = true;
      },
    };
  }

  /**
   * Build GenerateContentParameters from our request format
   */
  private async buildGenerateContentParams(
    request: BaseModelRequest | ExtendedModelRequest
  ): Promise<GenerateContentParameters> {
    const isExtended = 'userMessage' in request;
    const model = request.model || 'gemini-2.0-flash-exp';

    // Build system instruction
    let systemInstruction = '';
    if (isExtended) {
      const extReq = request as ExtendedModelRequest;

      // Load AGENTS.md memory if available
      let agentsMemory: string | null = null;
      if (this.plugin?.agentsMemory) {
        try {
          agentsMemory = await this.plugin.agentsMemory.read();
        } catch (error) {
          console.warn('Failed to load AGENTS.md:', error);
        }
      }

      // Build unified system prompt with tools, custom prompt, and agents memory
      // This includes: base system prompt + vault context (AGENTS.md) + tool instructions (if tools) + custom prompt (if provided)
      systemInstruction = this.prompts.getSystemPromptWithCustom(
        extReq.availableTools,
        extReq.customPrompt,
        agentsMemory
      );

      // Append additional instructions from prompt field (e.g., generalPrompt, contextPrompt)
      // Only append if custom prompt didn't override everything
      if (extReq.prompt && !extReq.customPrompt?.overrideSystemPrompt) {
        systemInstruction += `\n\n${extReq.prompt}`;
      }
    } else {
      // For BaseModelRequest, prompt is the full input
      systemInstruction = request.prompt || '';
    }

    // Build conversation contents
    const contents = this.buildContents(request);

    // Build config
    const config: GeminiGenerationConfig = {
      temperature: request.temperature ?? this.config.temperature,
      topP: request.topP ?? this.config.topP,
      ...(this.config.maxOutputTokens && { maxOutputTokens: this.config.maxOutputTokens }),
      ...(systemInstruction && { systemInstruction }),
    };

    // Add function calling tools
    const hasTools = isExtended && (request as ExtendedModelRequest).availableTools?.length;
    if (hasTools) {
      const tools = (request as ExtendedModelRequest).availableTools || [];
      const functionDeclarations: FunctionDeclaration[] = tools.map((tool) => ({
        name: tool.name,
        description: tool.description,
        parameters: {
          type: Type.OBJECT,
          properties: (tool.parameters.properties || {}) as unknown as Record<string, Schema>,
          required: tool.parameters.required || [],
        },
      }));

      config.tools = config.tools || [];
      config.tools.push({ functionDeclarations: functionDeclarations as any });
    }

    // Build params
    // If no contents built, use a simple string from the prompt
    let finalContents: Content[] | string = contents;
    if (contents.length === 0 && !isExtended) {
      // For BaseModelRequest with no conversation, just pass the prompt as string
      finalContents = request.prompt || '';
    } else if (contents.length === 0) {
      // For ExtendedModelRequest with no history, create a simple user message
      const extReq = request as ExtendedModelRequest;
      finalContents = extReq.userMessage || '';
    }

    const params: GenerateContentParameters = {
      model,
      contents: finalContents,
      config,
    };

    return params;
  }

  /**
   * Build Content[] array from request
   */
  private buildContents(request: BaseModelRequest | ExtendedModelRequest): Content[] {
    if (!('userMessage' in request)) {
      // BaseModelRequest - just send the prompt as user message
      if (!request.prompt) return [];
      return [
        {
          role: 'user',
          parts: [{ text: request.prompt }],
        },
      ];
    }

    const extReq = request as ExtendedModelRequest;
    const contents: Content[] = [];

    // Add conversation history
    if (extReq.conversationHistory?.length) {
      for (const entry of extReq.conversationHistory) {
        // Support Content format (already has role and parts)
        if ('role' in entry && 'parts' in entry) {
          contents.push(entry as Content);
        }
        // Support our internal format with role and text
        else if ('role' in entry && 'text' in entry) {
          contents.push({
            role: entry.role === 'user' ? 'user' : 'model',
            parts: [{ text: entry.text }],
          });
        }
        // Support our internal format with role and message
        else if ('role' in entry && 'message' in entry) {
          const msg = entry as ConversationEntryWithMessage;
          contents.push({
            role: msg.role === 'user' ? 'user' : 'model',
            parts: [{ text: msg.message }],
          });
        }
      }
    }

    // Add current user message (only if non-empty)
    if (extReq.userMessage?.trim()) {
      contents.push({
        role: 'user',
        parts: [{ text: extReq.userMessage }],
      });
    }

    return contents;
  }

  /**
   * Extract ModelResponse from GenerateContentResponse
   */
  private extractModelResponse(response: GenerateContentResponse): ModelResponse {
    let markdown = '';
    let rendered = '';
    const toolCalls: ToolCall[] | undefined = this.extractToolCallsFromResponse(response);

    // Extract text from candidates
    if (response.candidates?.[0]?.content?.parts) {
      for (const part of response.candidates[0].content.parts) {
        if ('text' in part && part.text) {
          markdown += part.text;
        }
      }
    }

    // Extract search grounding
    rendered = this.extractRenderedFromResponse(response);

    return {
      markdown,
      rendered,
      ...(toolCalls && { toolCalls }),
    };
  }

  /**
   * Extract text from streaming chunk
   */
  private extractTextFromChunk(chunk: GenerateContentResponse): string {
    if (chunk.candidates?.[0]?.content?.parts) {
      return chunk.candidates[0].content.parts
        .filter((part: Part) => 'text' in part && (part as ExtendedPart).text)
        .map((part: Part) => (part as ExtendedPart).text || '')
        .join('');
    }
    return '';
  }

  /**
   * Extract tool calls from response
   */
  private extractToolCallsFromResponse(response: GenerateContentResponse): ToolCall[] | undefined {
    const parts = response.candidates?.[0]?.content?.parts;
    if (!parts) return undefined;

    const toolCalls: ToolCall[] = [];
    for (const part of parts) {
      if ('functionCall' in part && part.functionCall && part.functionCall.name) {
        toolCalls.push({
          name: part.functionCall.name,
          arguments: (part.functionCall.args || {}) as Record<string, any>,
        });
      }
    }

    return toolCalls.length > 0 ? toolCalls : undefined;
  }

  /**
   * Extract tool calls from streaming chunk
   */
  private extractToolCallsFromChunk(chunk: GenerateContentResponse): ToolCall[] | undefined {
    return this.extractToolCallsFromResponse(chunk);
  }

  /**
   * Extract rendered HTML from response (search grounding)
   */
  private extractRenderedFromResponse(response: GenerateContentResponse): string {
    // Search grounding metadata is in groundingMetadata
    const metadata = (response as ExtendedGenerateContentResponse).candidates?.[0]?.groundingMetadata;
    if (!metadata) return '';

    // Extract and format grounding sources
    const chunks = metadata.groundingChunks || [];
    const _supports = metadata.groundingSupports || [];

    if (chunks.length === 0) return '';

    // Build HTML similar to how Gemini API returns it
    let html = '<div class="search-grounding">';
    html += '<h4>Sources:</h4>';
    html += '<ul>';

    for (const chunk of chunks) {
      if (chunk.web) {
        html += `<li><a href="${chunk.web.uri}" target="_blank">${chunk.web.title || chunk.web.uri}</a></li>`;
      }
    }

    html += '</ul>';
    html += '</div>';

    return html;
  }

  /**
   * Extract rendered content from streaming chunk
   */
  private extractRenderedFromChunk(chunk: GenerateContentResponse): string {
    return this.extractRenderedFromResponse(chunk);
  }

  /**
   * Generate an image from a text prompt
   * @param prompt - Text description of the image to generate
   * @param model - Image generation model (defaults to gemini-2.5-flash-image-preview)
   * @returns Base64 encoded image data
   */
  async generateImage(prompt: string, model: string = 'gemini-2.5-flash-image-preview'): Promise<string> {
    try {
      const params: GenerateContentParameters = {
        model,
        contents: prompt,
        config: {
          // Image generation typically doesn't need temperature/topP
          // but we can include them if needed
        },
      };

      const response = await this.ai.models.generateContent(params);

      // Extract base64 image data from response
      // The response may contain multiple parts: text + inlineData
      // We need to find the part with inlineData
      const parts = response.candidates?.[0]?.content?.parts;
      if (!parts || parts.length === 0) {
        throw new Error('No content parts in response');
      }

      // Find the part with image data
      for (const part of parts) {
        if ('inlineData' in part && part.inlineData?.data) {
          return part.inlineData.data;
        }
      }

      // If we get here, no image data was found
      throw new Error('No image data in response. The model may have returned only text.');
    } catch (error) {
      console.error('[GeminiClient] Error generating image:', error);
      // Log additional details if available
      if (error && typeof error === 'object') {
        console.error('[GeminiClient] Error details:', JSON.stringify(error, null, 2));
      }
      throw error;
    }
  }
}
